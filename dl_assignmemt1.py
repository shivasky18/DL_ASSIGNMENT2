# -*- coding: utf-8 -*-
"""DL_ASSIGNMEMT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aBbWmWBGdxqAiMrifJZ6mwQwUYh4SQZf

NEW SONG LYRICS GENERATOR USING GPT2
"""

!pip install nbstripout

!pip install datasets

import kagglehub

# Download latest version
path = kagglehub.dataset_download("paultimothymooney/poetry")

print("Path to dataset files:", path)

import numpy as np
import pandas as pd
import tensorflow as tf
from datasets import load_dataset
from transformers import GPT2TokenizerFast, DataCollatorForLanguageModeling

# Load the lyrics file for the chosen artist
file_path = "/kaggle/input/poetry/Kanye_West.txt"  # Load the chosen artist/band file
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()
    #print(f"Total words (unique tokens): {text}")

# Initialize the tokenizer
model_id = "gpt2-medium"
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

# Define constants
MAX_LENGTH = 256
BATCH_SIZE = 2  # Define the batch size

# Tokenize the entire corpus
tokenized_data = tokenizer(
    text,
    truncation=True,
    max_length=MAX_LENGTH,
    return_overflowing_tokens=True,
    return_length=True
)

# Check the tokenized data
print(tokenized_data)

from transformers import GPT2TokenizerFast

# Load the lyrics file for the chosen artist
file_path = '/kaggle/input/poetry/michael-jackson.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# Initialize the tokenizer
model_id = "gpt2-medium"
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

# Define constants
MAX_LENGTH = 256
BATCH_SIZE = 2  # Define the batch size

# Tokenize the entire corpus
tokenized_data = tokenizer(
    text,
    truncation=True,
    max_length=MAX_LENGTH,
    return_overflowing_tokens=True,
    return_length=True
)

# Check the tokenized data
print(tokenized_data)

def preprocess(example):
    try:
        outputs = tokenizer(
            example,
            truncation=True,
            max_length=MAX_LENGTH,
            return_overflowing_tokens=True,
            return_length=True,
        )
        input_batch = []
        for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
            if length == MAX_LENGTH:
                input_batch.append(input_ids)
                valid_input_ids = input_ids
        if len(input_batch) != 0:
            for i in range(BATCH_SIZE - len(input_batch)):
                input_batch.append(valid_input_ids)
    except Exception as e:
        input_batch = []
    return {"input_ids": input_batch}

preprocessed_data = preprocess(text)
print(preprocessed_data)

# Convert the tokenized data into a list
input_ids = preprocessed_data['input_ids']
dataset_list = [{'input_ids': input_id} for input_id in input_ids]

# Check the length of the dataset
num_train_steps = len(dataset_list) * BATCH_SIZE
print(f'Number of training steps: {num_train_steps}')

import tensorflow as tf

# Convert the list back to a TensorFlow dataset
tf_train_dataset = tf.data.Dataset.from_generator(
    lambda: (example for example in dataset_list),
    output_signature={
        'input_ids': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32)
    }
)

# Create batches
tf_train_dataset = tf_train_dataset.batch(BATCH_SIZE).map(lambda x: {
    'input_ids': tf.stack(x['input_ids']),
    'attention_mask': tf.ones([BATCH_SIZE, MAX_LENGTH], dtype=tf.int32),
    'labels': tf.stack(x['input_ids'])
})

# Check the TensorFlow dataset
for i in tf_train_dataset.take(1):
    print(i)

from transformers import TFGPT2LMHeadModel, create_optimizer

# Load the GPT-2 model
model = TFGPT2LMHeadModel.from_pretrained(model_id)

# Create an optimizer
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps
)

# Compile the model
model.compile(optimizer=optimizer)

import tensorflow as tf

# Convert the list back to a TensorFlow dataset
tf_train_dataset = tf.data.Dataset.from_generator(
    lambda: (example for example in dataset_list),
    output_signature={
        'input_ids': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32)
    }
)

# Create batches
tf_train_dataset = tf_train_dataset.batch(BATCH_SIZE).map(lambda x: {
    'input_ids': tf.stack(x['input_ids']),
    'attention_mask': tf.ones([BATCH_SIZE, MAX_LENGTH], dtype=tf.int32),
    'labels': tf.stack(x['input_ids'])  # The labels should be of size [BATCH_SIZE, MAX_LENGTH]
})
# Do not squeeze the labels, as they should have shape (BATCH_SIZE, MAX_LENGTH)
# tf_train_dataset = tf_train_dataset.map(lambda x: {
#     'input_ids': x['input_ids'],
#     'attention_mask': x['attention_mask'],
#     'labels': tf.squeeze(x['labels'], axis=0)  # Remove the extra dimension (axis=0) - THIS IS THE PROBLEM
# })

# Check the TensorFlow dataset
for i in tf_train_dataset.take(1):
    print(i)

import time

input_text = "true love shouldn't be this complicated"
input_ids = tokenizer(input_text, return_tensors="tf")["input_ids"]

# Generate text
init_time = time.time()
output_temp = model.generate(input_ids, max_length=256, do_sample=True, temperature=1.0, top_k=50)
print(tokenizer.decode(output_temp[0]))
print(time.time() - init_time)

"""RNN based seq2seq model on Dakshina dataset"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
import random

train_path = '/content/hi.translit.sampled.train.tsv'
test_path = '/content/hi.translit.sampled.test.tsv'

# Load data assuming TSV format with columns: target, input, count
train_df = pd.read_csv(train_path, sep='\t', header=None, names=['target', 'input', 'count'])
test_df = pd.read_csv(test_path, sep='\t', header=None, names=['target', 'input', 'count'])

# Optional: create validation split from training data
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)

def build_vocab(sequences):
    # Filter out any non-string values before building the vocabulary
    sequences = [seq for seq in sequences if isinstance(seq, str)]
    chars = set(char for seq in sequences for char in seq)
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}
    vocab.update({char: i+3 for i, char in enumerate(sorted(chars))})
    return vocab

input_vocab = build_vocab(train_df['input'])
target_vocab = build_vocab(train_df['target'])

inv_target_vocab = {v: k for k, v in target_vocab.items()}

class TransliterationDataset(Dataset):
    def __init__(self, data, input_vocab, target_vocab):
        self.data = data
        self.input_vocab = input_vocab
        self.target_vocab = target_vocab

    def encode_seq(self, seq, vocab, add_sos_eos=False):
        # Convert seq to string if it's not already
        if not isinstance(seq, str):
            seq = str(seq)
        ids = [vocab[char] for char in seq]
        if add_sos_eos:
            ids = [vocab['<sos>']] + ids + [vocab['<eos>']]
        return torch.tensor(ids)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        # Ensure input is a string by converting it if necessary
        input_seq = self.encode_seq(str(row['input']), self.input_vocab)
        target_seq = self.encode_seq(str(row['target']), self.target_vocab, add_sos_eos=True)
        return input_seq, target_seq

def collate_fn(batch):
    input_seqs, target_seqs = zip(*batch)
    input_lens = [len(seq) for seq in input_seqs]
    target_lens = [len(seq) for seq in target_seqs]
    input_pad = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=0)
    target_pad = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=0)
    return input_pad, target_pad, input_lens, target_lens

train_ds = TransliterationDataset(train_df, input_vocab, target_vocab)
val_ds = TransliterationDataset(val_df, input_vocab, target_vocab)
test_ds = TransliterationDataset(test_df, input_vocab, target_vocab)

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)
test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)

class Seq2Seq(nn.Module):
    def __init__(self, input_dim, target_dim, emb_dim, hidden_dim, rnn_type='GRU', num_layers=1):
        super().__init__()
        self.encoder_embed = nn.Embedding(input_dim, emb_dim)
        self.decoder_embed = nn.Embedding(target_dim, emb_dim)

        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[rnn_type]
        self.encoder = rnn_cls(emb_dim, hidden_dim, num_layers, batch_first=True)
        self.decoder = rnn_cls(emb_dim, hidden_dim, num_layers, batch_first=True)

        self.fc_out = nn.Linear(hidden_dim, target_dim)
        self.rnn_type = rnn_type
        self.hidden_dim = hidden_dim

    def forward(self, src, tgt):
        # Encoder
        src_emb = self.encoder_embed(src)
        _, hidden = self.encoder(src_emb)

        # Decoder
        tgt_emb = self.decoder_embed(tgt[:, :-1])
        outputs, _ = self.decoder(tgt_emb, hidden)
        logits = self.fc_out(outputs)
        return logits

def train(model, loader, optimizer, loss_fn, device):
    model.train()
    total_loss = 0
    for inputs, targets, _, _ in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        output = model(inputs, targets)
        loss = loss_fn(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader, loss_fn, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for inputs, targets, _, _ in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            output = model(inputs, targets)
            loss = loss_fn(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))
            total_loss += loss.item()
    return total_loss / len(loader)

def predict(model, dataset, device):
    model.eval()
    predictions = []
    with torch.no_grad():
        for input_seq, _ in dataset:
            input_seq = input_seq.unsqueeze(0).to(device)
            embedded = model.encoder_embed(input_seq)
            _, hidden = model.encoder(embedded)

            decoder_input = torch.tensor([[target_vocab['<sos>']]], device=device)
            output_seq = []
            for _ in range(30):  # max length
                emb = model.decoder_embed(decoder_input)
                out, hidden = model.decoder(emb, hidden)
                logits = model.fc_out(out.squeeze(1))
                pred_token = logits.argmax(-1).item()
                if pred_token == target_vocab['<eos>']:
                    break
                output_seq.append(inv_target_vocab[pred_token])
                decoder_input = torch.tensor([[pred_token]], device=device)
            predictions.append("".join(output_seq))
    return predictions

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Seq2Seq(
    input_dim=len(input_vocab),
    target_dim=len(target_vocab),
    emb_dim=64,
    hidden_dim=128,
    rnn_type='GRU',
    num_layers=1
).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss(ignore_index=0)

# Training loop
for epoch in range(1, 21):
    train_loss = train(model, train_loader, optimizer, loss_fn, device)
    val_loss = evaluate(model, val_loader, loss_fn, device)
    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")

# Predict on test set
test_predictions = predict(model, test_ds, device)
for i in range(20):
    print(f"Latin: {test_df.iloc[i]['input']} â†’ Predicted Devanagari: {test_predictions[i]} | Actual: {test_df.iloc[i]['target']}")

# =================== Accuracy ====================
def compute_accuracy(predictions, targets):
    correct = 0
    total = len(predictions)
    for pred, actual in zip(predictions, targets):
        if pred.strip() == actual.strip():
            correct += 1
    return correct / total

# Get actual Devanagari targets
actual_targets = list(test_df['target'])

# Calculate accuracy
accuracy = compute_accuracy(test_predictions, actual_targets)
print(f"\nTest Accuracy: {accuracy * 100:.2f}%")